<base href="http://www.rybak-et-al.net/vnc.html">
<html>

<head>
<meta name="keywords"
content="Ilya Rybak, Rybak, vision, biological vision,

computer vision, pattern recognition, image recognition, invariant image 

recognition, active visual perception, visual perception, visual cortex, 

recognition, visual processing, image representation, attention, visual 

features, eye movements, foveation, fovea, high level vision, what and where 

pathways, scanpath, cognition, neural networks, modeling, model, simulations,  

neural computations">
<title>BMV: Behavioral model of visual perception and recognition</title>
</head>

<body BGCOLOR="#FFFFFF">

<h3 align="center"><big>BMV:&nbsp; Behavioral Model of Visual Perception and Recognition </big></h3>

<h3 align="center"><a href="index.html">I. A. Rybak </a>, <a
href="http://nisms.krinc.ru/index_en.html">V. I. Gusakova, A. V. Golovan,<br>
L. N. Podladchikova, and N. A. Shevtsova </a></h3>
<strong><a href="http://www.krinc.ru/w715/News_eng.htm">

<p align="center">A. B. Kogan Research Institute for Neurocybernetics</a><br>
Rostov State University, Rostov-on-Don, Russia</strong> </p>

<p align="center"><strong><i><font color="ff0000">The mind can only see what it is
prepared to see. </font></i><br>
Edward de Bono</strong></p>

<p align="center"><img src="fuchsimg4.gif" width="143" height="195"
alt="fuchsimg4.gif (35573 bytes)"><br>
<small>This animated Yarbus' figure is taken from </small><br>
<small>the page of Dr.Albert Fuchs (U. Washington)</small><br>
</p>
<a name="APPROACH">

<h3><font color="#FF0000"><big>Approach</big></font></h3>
</a><a href="yarbus.gif">

<p><img src="yarbus.gif" width="243" height="173" align="right" hspace="5" vspace="5"></a>
During visual perception and recognition, human eyes move and successively fixate at the
most informative parts of the image (Yarbus 1967, see illustrations above and on the
right.). The eyes actively perform problem-oriented selection and processing of
information from the visible world under the control of visual attention (Burt 1988;
Julesz 1975; Neisser 1967; Noton and Stark 1971; Triesman and Gedal 1980, Yarbus 1967).
Consequently, visual perception and recognition may be considered as behavioral processes,
and probably cannot be completely understood in limited frames of neural computations
without taking into account behavioral and cognitive aspects of these processes. </p>

<p>From the behavioral point of view, an internal representation (model) of new
circumstances is formed in the brain during conscious observation and active examination.
The active examination is aimed toward the finding and memorizing of functional
relationships between the applied actions and the resulting changes in sensory
information. An external object becomes &quot;known&quot; and may be recognized when the
system is able to subconsciously manipulate the object and to predict the object's
reactions to the applied actions. According to this paradigm, the internal object
representation contains chains of alternating traces in &quot;motor&quot; and
&quot;sensory&quot; memories. Each of these chains reflects an alternating sequence of
elementary motor actions and sensory (proprioceptive and external) signals which are
expected to arrive in response to each action. The brain uses these chains as
&quot;behavioral programs&quot; in subconscious &quot;behavioral recognition&quot; when
the object is (or is assumed) known. This &quot;behavioral recognition&quot; has two basic
stages: (i) conscious selection of the appropriate behavioral program (when the system
accepts a hypothesis about the object), and (ii) subconscious execution of the program.
Matching the expected (predicted) sensory signals to the actual sensory signals, arriving
after each motor action, is an essential operation in the program execution. </p>

<p>The above <i>behavioral paradigm</i> was formulated and developed in the context of
visual perception and recognition in a series of significant works (Yarbus, 1967; Noton
&amp; Stark, 1971; Didday &amp; Arbib, 1975; Kosslyn et al., 1990; Rimey and Brown, 1991).
Using Yarbus' approach, Noton and <a
HREF="http://www.universityofcalifornia.edu/senate/inmemoriam/lawrencestark.htm">Stark</a>
(1971) compared the individual scanpaths of human eye movements in two phases: during
image memorizing, and during the subsequent recognition of the same image. They found
these scanpaths to be topologically similar and suggested that each object is memorized
and stored in memory as <a href="stark2.gif">an alternating sequence of object features
and eye movements required to reach the next feature</a>. The results of Noton and Stark
(1971) and Didday and Arbib (1975) prompted the consideration of eye movement scanpaths as
behavioral programs for recognition. The process of recognition was supposed to consist of
an alternating sequence of eye movements (recalled from the motor memory and directed by
attention) and verifications of the expected image fragments (recalled from the sensory
memory). </p>

<p><a href="ww.gif"><img src="ww.gif" width="250" height="178" align="right" hspace="5"
vspace="5"></a> Ungerleider and Mishkin (1982), Mishkin, Ungerleider and Macko (1983), Van
Essen (1985), and Kosslyn et al. (1990) presented neuro-anatomical and psychological data
complementary to the above behavioral concept. It was found that the higher levels of the
visual system contain two major pathways for visual processing called &quot;where&quot;
and &quot;what&quot; pathways. The &quot;where&quot; pathway leads dorsally to the
parietal cortex and is involved in processing and representing spatial information
(spatial locations and relationships). The &quot;what&quot; pathway leads ventrally to the
inferior temporal cortex and deals with processing and representing object features. </p>

<p>Our approach is based on the above behavioral, psychological and anatomical concepts.
We propose that invariant object recognition in human vision is provided by the following:

<ul>
  <p><font color="ff000">(i) </font>separated processing and representation of
  &quot;what&quot; (object features) and &quot;where&quot; (spatial features: elementary eye
  movements) information at the high levels of the visual system; </p>
  <p><font color="ff000">(ii) </font>using a frame of reference attached to the
  &quot;basic&quot; feature at each fixation point for the invariant encoding of
  &quot;what&quot; and &quot;where&quot; pieces of information, i.e. a <font color="#FF0000"><em>feature-based
  frame of reference</em></font>;</p>
  <p><font color="#FF0000">(iii) </font>testing a hypothesis formed at single fixation
  during a series of consequent fixations under top-down control of attention;</p>
  <p><font color="ff000">(iv) </font>mechanisms of visual attention that use
  &quot;where&quot; information stored in the memory to direct sequential image processing
  (hypothesis testing); </p>
  <p><font color="ff000">(v) </font>mechanisms which provide matching the current object
  features to the expected features ( &quot;what&quot; information stored in the memory) at
  each fixation. </p>
</ul>
<a name="MODEL">

<h3><big><font color="#FF0000">Model</font></big></h3>

<p>A functional diagram of the model is shown in Figure 1 (see below). The attention
window (AW) performs a primary transformation of the image into a &quot;retinal
image&quot; at the fixation point. The primary transformation provides a decrease in
resolution of the retinal image from the center to the periphery of the AW that simulates
the decrease in resolution from the fovea to the retinal periphery in the cortical map of
the retina. (An example of the &quot;retinal image&quot; is shown in </a><a name="FIG2"></a><a
href="bmvf2.html"> Figure 2</a>). </p>

<p>The retinal image in the AW is used as input to the module for primary feature
detection which performs a function similar to the primary visual cortex. This module
contains a set of neurons with orientationally selective receptive fields (ORF) tuned to
different orientations of the local edge. Neurons with the ORF, centered at the same point
but with different orientation tuning, interact competitively due to strong reciprocal
inhibitory interconnections. The orientation tuning of the &quot;winning neuron&quot;
encodes the edge orientation at each point. In each fixation (AW position), the module for
primary feature detection extracts a set of edges. This set includes a &quot;basic&quot;
edge located at the fixation point (center of AW) and several &quot;context&quot; edges
located at specific positions in the retinal image. Thus, the module for primary feature
encoding represents the image fragment at the current fixation point by the set of
oriented edges extracted with resolution decreased to the periphery of AW. (An example is
shown in <a name="FIG3"></a><a href="bmvf3.html"> Figure 3</a>). </p>

<p>The modules described above form a low-level subsystem of the model. The next module
performs a mid-level processing. It transforms the set of primary features into the
invariant second-order features using a coordinate system (frame of reference) attached to
the basic edge in the center of the AW and oriented along the brightness gradient of the
basic edge. The <i>relative orientations</i> and <i>relative angular locations</i> of the
context edges with respect to the basic edge are considered as invariant second-order
features. </p>

<p align="center"><img src="dia.gif" width="459" height="384"> </p>

<h4 align="center">Fig. 1 Schematic of the model.</h4>

<p align="left">The performance of the high-level subsystem and the entire model may be
considered in three different modes: memorizing, search, and recognition. </p>

<table border="0" width="100%">
  <tr>
    <td width="43%">In the <i>memorizing mode</i>, the image is processed at sequentially
    selected fixation points. At each fixation point, the set of edges is extracted from the
    AW, transformed into the invariant second-order features and stored in the sensory memory
    (&quot;what&quot; structure). The next position of the AW (next fixation point) is
    selected from the set of context points (see an example in <a name="FIG4"></a><a
    href="bmvf4.html"> Figure 4</a>) and is represented in the coordinate system attached to
    the basic edge. A special module shifts the AW to a new fixation point via the AW
    controller playing the role of the oculomotor system. Each relative shift of the AW
    (&quot;eye movement&quot;) is stored in the motor memory (&quot;where&quot;-structure).</td>
    <td width="57%"><p align="center"><img src="lena2.gif" width="163" height="163">&nbsp; <img
    src="kor.gif" width="163" height="163"><br>
    <strong><small>Illustrations of parallel-sequential image processing <br>
    during the memorizing and recognition modes </small></strong></td>
  </tr>
</table>

<p>(Click on <a href="bmvf5.html">Figure 5 </a>to see an example of the scanpath of
viewing during the memorizing mode). As a result of the memorizing mode, the whole
sequence of retinal images is stored in the &quot;what&quot; structure (sensory memory),
and the sequence of AW movements is stored in the &quot;where&quot; structure (motor
memory). These two types of elementary &quot;memory traces&quot; alternate in a chain
which is considered as a &quot;behavioral recognition program&quot; for the memorized
image. </p>

<p>In the <i>search mode</i>, the image is scanned by the AW under the control of a search
algorithm. At each fixation, the current retinal image from the AW is compared to all
retinal images of all objects stored in the sensory memory. The scanning of the image
continues until a retinal image similar to one of the stored retinal images is found at
some fixation point. When such a retinal image is found, a hypothesis about the image is
formed, and the model turns to the recognition mode. </p>

<p>In the <i>recognition mode</i>, the behavioral program is executed by consecutive
shifts of the AW (controlled by the AW controller using data recalled from the motor
memory) and consecutive verification of the expected retinal images recalled from the
sensory memory. The scanpath of viewing in the recognition mode reproduces sequentially
the scanpath of viewing in the memorizing mode. If a series of successful matches occurs,
the object is recognized, otherwise the model returns to the search mode. </p>

<p>Our simulation showed that the model can recognize complex gray-level images (e.g.
faces) invariantly with respect to shift, rotation, and scale. Click below to see examples
of invariant recognition of scene objects and faces. Examples of recognition of <a
href="example1.html">scene objects </a>and <a href="examples.html">faces </a><br>
</p>

<p><font color="#FF0000"><big><big><strong>Publications</strong></big></big></font></p>

<p>Rybak, I. A., Gusakova, V. I., Golovan, A. V., Podladchikova, L. N., and Shevtsova,
&nbsp;N. A. (1998) A model of attention-guided visual perception and recognition. <em>Vision
Research</em> 38: 2387-2400 (<a
href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed&amp;cmd=Retrieve&amp;dopt=AbstractPlus&amp;list_uids=9798006&amp;query_hl=2&amp;itool=pubmed_docsum">Medline</a>)(<a
href="http://neurobio.drexelmed.edu/Rybakweb/vr.pdf">pdf</a>). </p>

<p>Rybak, I. A., Gusakova, V. I., Golovan, A. V., Podladchikova, L. N., and Shevtsova, N.
A. (2005) Attention-guided recognition based on &#147;what&#148; and &#147;where&#148;
representations: A behavioral model. In: <a
href="http://ilab.usc.edu/publications/doc/NeurobiologyOfAttention/"><em>Neurobiology of
Attention</em> (Eds. Itti, L., Rees, G. and Tsotsos, J.). Elsevier Acad. Press</a>, pp.
663-670 (<a href="http://neurobio.drexelmed.edu/Rybakweb/NoA.pdf">pdf</a>).</p>

<p align="center"><a href="http://globetrotter.berkeley.edu/conversations/Stark/"><img
src="StarkConHead.jpg" width="489" height="200" alt="StarkConHead.jpg (23704 bytes)"></a><br>
<a href="http://www.universityofcalifornia.edu/senate/inmemoriam/lawrencestark.htm"><img
src="Stark1.jpg" width="156" height="235" alt="Stark1.jpg (11746 bytes)">&nbsp; <img
src="Stark2.jpg" width="156" height="235" alt="Stark2.jpg (11440 bytes)">&nbsp; <img
src="Stark3.jpg" width="156" height="235" alt="Stark3.jpg (11947 bytes)"><br>
<big>In memory of Professor Lawrence Stark</big></a></p>

<p align="center"><img src="h2x6b.gif" width="540" height="4"> </p>

<p align="center"><br>
<a href="http://nisms.krinc.ru/index_en.html"><img src="krinc.JPG" width="69" height="52"
alt="krinc.JPG (3010 bytes)"></a>&nbsp;&nbsp; <a href="rybak.html"><img src="mylogo-1.jpg"
width="333" height="52" alt="mylogo-1.jpg (18590 bytes)"></a> &nbsp; <br>
<a href="mailto:nisms@krinc.ru">nisms@krinc.ru </a><br>
<a href="mailto:ilya.rybak@drexelmed.edu">ilya.rybak@drexelmed.edu</a><br>
<a href="mailto:ilya@rybak-et-al.net">ilya@rybak-et-al.net</a><br>
</p>
</body>
</html>
